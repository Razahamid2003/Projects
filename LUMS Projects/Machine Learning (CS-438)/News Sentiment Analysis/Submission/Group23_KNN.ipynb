{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Model Implementation (K Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been split using sklearn into Training and Testing data in a 70-30 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('merged_file.csv')\n",
    "texts = data['Cleaned_content']\n",
    "labels = data['Gold Label']\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use this Function to build a vocabulary list of words from the texts that are given to us.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(texts):\n",
    "    vocabulary = set()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        vocabulary.update(words)\n",
    "    return sorted(list(vocabulary))\n",
    "vocabulary = build_vocabulary(X_train_texts)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code calculates the Inverse Document Frequency (IDF) for each word in a given vocabulary based on a set of texts. IDF is a key component of the TF-IDF (Term Frequency-Inverse Document Frequency) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(texts, vocabulary):\n",
    "    N = len(texts)\n",
    "    idf = np.zeros(len(vocabulary))\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        df = sum(1 for text in texts if word in text.split())\n",
    "        print()\n",
    "        idf[idx] = math.log((N + 1) / (df + 1)) + 1  \n",
    "    return idf\n",
    "idf = compute_idf(X_train_texts, vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert text data into numerical representations using TF-IDF vectors, which can be directly used as input to our KNN Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text, vocabulary, idf):\n",
    "    tf = np.zeros(len(vocabulary))\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    for word in words:\n",
    "        idx = word_to_index.get(word)\n",
    "        if idx is not None:\n",
    "            tf[idx] = word_counts[word]\n",
    "    tfidf = tf * idf\n",
    "    norm = np.linalg.norm(tfidf)\n",
    "    if norm != 0:\n",
    "        tfidf = tfidf / norm\n",
    "    return tfidf\n",
    "\n",
    "X_train_vectors = np.array([text_to_vector(text, vocabulary, idf) for text in X_train_texts])\n",
    "X_test_vectors = np.array([text_to_vector(text, vocabulary, idf) for text in X_test_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to compute the cosine similarity between two vectors which is a commonly used metric in text processing and machine learning, especially in models like K-Nearest Neighbors, where distance or similarity between feature vectors matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(test_vector, train_vectors, train_labels, k):\n",
    "    similarities = []\n",
    "    for idx, train_vector in enumerate(train_vectors):\n",
    "        similarity = cosine_similarity(test_vector, train_vector)\n",
    "        similarities.append((similarity, train_labels.iloc[idx]))\n",
    "    \n",
    "    similarities.sort(reverse=True, key=lambda x: x[0])    \n",
    "    top_k = similarities[:k]\n",
    "    top_k_labels = [label for _, label in top_k]\n",
    "    label_counts = Counter(top_k_labels)\n",
    "    predicted_label = label_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "predictions = []\n",
    "\n",
    "for test_vector in X_test_vectors:\n",
    "    predicted_label = knn_predict(test_vector, X_train_vectors, y_train, k)\n",
    "    predictions.append(predicted_label)\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions, labels=data['Gold Label'].unique())\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=data['Gold Label'].unique(), yticklabels=data['Gold Label'].unique())\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
